{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taaltheorie en Taalverwerking · 2019 · Week 20\n",
    "\n",
    "In this assignment, we will work more with WordNet and explore music processing with NLTK tools. Don't forget to load WordNet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILL THIS IN FOR YOUR GROUP, also name your file as: tttv-w20-<group>-<name1>-<name2>.ipynb\n",
    "\n",
    "# Group        : D\n",
    "# Name - UvaID : Joshua de Roos - 11242736\n",
    "# Name - UvaID : Lodewijk van Keizerswaard - 11054115\n",
    "# Date         : 24-05-2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/joshua/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import *\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will also need to install and use the [**requests**](https://2.python-requests.org/en/master/) library for this assignment, either by typing `pip install requests` in a terminal or by using your Python package manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 (11 pts total)\n",
    "\n",
    "In this exercise you will explore the output of a distributional semantic model and compare its semantic similarity ranking to that obtained with path-length distance in an ontology. \n",
    "\n",
    "[Indra](http://lambda3.org/Indra/) is a library for working with several distributional semantic models and includes a number of pre-trained models that can be queried online. For a target word, Indra will return its $n$ nearest semantic neighbours ordered by similarity strength, calculated using the similarity measure of your choice.\n",
    "\n",
    "For this assignment, we will use the Word2Vec model on English-language corpora (`wiki-2018` and `googlenews`), and we will restrict ourselves to the five most similar words according to cosine similarity. We can make a query over `wiki-2018` for the word *sailboat* as follows. We will receive a dictionary containing the most similar words according to Word2Vec, as well as their cosine similarity to *sailboat*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.post('http://indra.lambda3.org/neighbors/relatedness', json = {\n",
    "        'corpus': 'wiki-2018',\n",
    "        'model': 'W2V',\n",
    "        'language': 'EN', \n",
    "        'topk': 5, \n",
    "        'scoreFunction': 'COSINE',\n",
    "        'terms': ['sailboat']\n",
    "})\n",
    "r.json()\n",
    "\n",
    "# {'corpus': 'wiki-2018',\n",
    "#  'model': 'W2V',\n",
    "#  'language': 'EN',\n",
    "#  'topk': 5,\n",
    "#  'terms': {'sailboat': {'sailboat': 0.9999999999999999,\n",
    "#    'catamaran': 0.7580487287345087,\n",
    "#    'trimaran': 0.7260682723499513,\n",
    "#    'dinghy': 0.7202068755210131,\n",
    "#    'multihull': 0.6924827761338878}}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1.1 (3 pts)\n",
    "\n",
    "Search for the target word *potato* using the same parameters as the example above. As output, you should get a list of five words including *potato*, with a similarity score that indicates how similar they are to *potato* according to the model. \n",
    "\n",
    "Use NLTK to determine the lowest common hypernym of these five words in *WordNet*.\n",
    "\n",
    "**Hint:** Don't forget that you need to check all possible senses for each word. The lowest common hypernym is the lowest-level synset that subsumes at least one sense of all five words. You probably *don't* want to use the NLTK **lowest_common_hypernyms()** method. (Can you see why?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1.2 (3 pts)\n",
    "\n",
    "Repeat the same exercise using the `googlenews` corpus instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1.3 (3 pts)\n",
    "\n",
    "Implement a function called **co\\_hyponym(node1, node2)** to check whether two *words* (not synsets) are co-hyponyms or sister terms (i.e., whether any sense of one of the words has some immediate hypernym in common with some sense of the other word). \n",
    "\n",
    "Which of the semantic neighbours on your lists from the previous two questions are co-hyponyms of *potato*? (Do not count *potatoes*, which has the same lemma as *potato*.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if they have the same parent.\n",
    "def co_hyponym(word1, word2):\n",
    "    pass\n",
    "\n",
    "# Make a print statement like the following for every neighbour.\n",
    "print(co_hyponym(\"potato\", \"potatoes\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1.4 (2 pts)\n",
    "\n",
    "Give the results of **textbook_similarity** queries between *potato* and each of the other semantic neighbours from previous questions, using your function from last week. Give the resulting list of semantic neighbours ordered by similarity strength (according to WordNet path-length) and compare this ranking to the rankings from Indra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textbook_similarity(word1, word2, pos):\n",
    "    # Copy your implementation from last week, or ask your TA for help if you had it wrong.\n",
    "    pass\n",
    "\n",
    "# Make a print statement like the following for every neighbour.\n",
    "print(textbook_similarity(\"potato\", \"potatoes\", pos = wn.NOUN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Question 2: Parsing Eurovision (7 pts total)\n",
    "\n",
    "This question will introduce the basics of grammar-based syntactic analysis of musical harmony.\n",
    "\n",
    "### Question 2.1 (1 pt)\n",
    "\n",
    "Go to http://chordify.net and run an analysis of France's official entry for the Eurovision Song Contest 2019 by copying the following link to the search: https://www.youtube.com/watch?v=dw7WqoSHtgU. Ignore the Eurovision branding at the beginning of the video. Starting from the entrance of the piano at the beginning of the song, copy the first 7 chords as they appear in the Chordify interface, as a list of Python strings. You may find it slightly faster if you change to the *Akkorden* view instead of *Diagrammen*. Use the '#' character for sharp signs.\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_chords = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.2 (3 pts)\n",
    "\n",
    "The following grammar is a (somewhat simplified) implementation of the harmonic grammar Fred Lerdahl proposes in his book *Tonal Pitch Space* (2001). In addition to the traditional harmonic classes of tonic, dominant, and subdominant harmony, Lerdahl's grammar includes a *departure* class (`Dep`), a *return* class (`Ret`), and a *neighbour* class (`N`). His definitions of the harmonic classes in terms of Roman numerals are commented out: the 'lexical rules' for this grammar.\n",
    "\n",
    "The tonal centre of France's Eurovision entry is F$\\sharp$ (Chordify gets it wrong). Use the principles we discussed in class to replace the Roman-numeral lexical rules with lexical rules for all of the chord symbols in `french_chords` (e.g., `T -> 'G'` for pieces in the key of G).\n",
    "\n",
    "Run the code block when you are finished to see how many parse trees Lerdahl's grammar can compute for this sentence.\n",
    "\n",
    "**Hint:** If you still find the rules for converting between chord names and Roman numerals confusing, try browsing Wikipedia's surprisingly good collection of articles on harmony, starting here: https://en.wikipedia.org/wiki/Roman_numeral_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import CFG\n",
    "from nltk.parse.chart import LeftCornerChartParser\n",
    "\n",
    "lerdahl_grammar = CFG.fromstring(\"\"\"\n",
    "  P -> T\n",
    "  T -> T T\n",
    "  T -> D T\n",
    "  D -> D D\n",
    "  D -> S D\n",
    "  S -> S S\n",
    "  T -> T N T\n",
    "  D -> D N D\n",
    "  S -> S N S\n",
    "  T -> T Dep\n",
    "  D -> D Dep\n",
    "  S -> S Dep\n",
    "  Dep -> N Dep\n",
    "  T -> Ret T\n",
    "  D -> Ret D\n",
    "  S -> Ret S\n",
    "  \n",
    "  # Replace the following strings with the actual chord symbols you need.\n",
    "  # \n",
    "  # T -> 'I'\n",
    "  # D -> 'V' | 'VII'\n",
    "  # S -> 'II' | 'III' | 'IV' | 'VI' | 'VII'\n",
    "  # Dep -> 'II' | 'III' | 'IV' | 'V' | 'VI' | 'VII' \n",
    "  # Ret -> 'II' | 'III' | 'IV' | 'V' | 'VI' | 'VII' \n",
    "  # N -> 'II' | 'III' | 'IV' | 'V' | 'VI' | 'VII'\n",
    "\"\"\")\n",
    "\n",
    "lerdahl_parser = LeftCornerChartParser(lerdahl_grammar)\n",
    "lerdahl_parses = lerdahl_parser.parse(french_chords)\n",
    "\n",
    "# Print the total number of parses as well as the actual parse trees.\n",
    "lerdahl_sum = 0\n",
    "for t in lerdahl_parses: \n",
    "    lerdahl_sum = lerdahl_sum + 1\n",
    "    t.pretty_print()\n",
    "print(lerdahl_sum, 'trees')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.3 (2 pts)\n",
    "\n",
    "The next grammar is based on the foundations of Chordify (Bas de Haas, *Music Information Retrieval Based on Tonal Harmony*, 2012). Chordify uses a different set of harmonic classes: tonic, dominant, subdominant, and *tonic prolongation* (`TPG`). They also define the members of the traditional classes differently than Fred Lerdahl; theirs  are based on a beautiful branch of music theory known as neo-Riemannian theory. Replace the commented 'lexical rules' in this grammar with actual chord symbols from `french_chords`, just like you did in the previous question, and run the block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chordify_grammar = CFG.fromstring(\"\"\"\n",
    "  P -> PCPa\n",
    "  P -> PCPb\n",
    "  P -> HCP\n",
    "  P -> P P\n",
    "\n",
    "  PCPa -> D T | D D T\n",
    "  PCPa -> PCPa T\n",
    "  PCPb -> T D T\n",
    "  HCP -> T D\n",
    "  HCP -> T HCP\n",
    "  D -> S D\n",
    "  T -> TPG\n",
    "  \n",
    "  T -> T T\n",
    "  D -> D D\n",
    "  S -> S S\n",
    "  \n",
    "  # Replace the following strings with the actual chord symbols you need.\n",
    "  # \n",
    "  # T -> 'I'\n",
    "  # D -> 'V' | 'VII'\n",
    "  # S -> 'II' | 'IV' \n",
    "  # TPG -> 'III' | 'VI'\n",
    "\"\"\")\n",
    "\n",
    "chordify_parser = LeftCornerChartParser(chordify_grammar)\n",
    "chordify_parses = chordify_parser.parse(french_chords)\n",
    "\n",
    "# Print the total number of parses as well as the actual parse trees.\n",
    "chordify_sum = 0\n",
    "for t in chordify_parses: \n",
    "    chordify_sum = chordify_sum + 1\n",
    "    t.pretty_print()\n",
    "print(chordify_sum, 'trees')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.4 (1 pt)\n",
    "\n",
    "Which grammar is more practical for daily use, Lerdahl's or Chordify's? Why?\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
