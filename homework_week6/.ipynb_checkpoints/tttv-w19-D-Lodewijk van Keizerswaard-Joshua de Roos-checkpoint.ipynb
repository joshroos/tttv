{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taaltheorie en Taalverwerking · 2019 · Assignment 5\n",
    "\n",
    "NLTK has a interface for working with WordNet in more detail than is possible with the web interface alone. In this assignment, we will explore that interface further. \n",
    "\n",
    "As you are becoming more comfortable with NLTK, this assignment will involve some searching through the documentation yourself in order to find the best functions or methods for your needs. Specifically, you will want to look at:\n",
    "\n",
    "  * *Natural Language Processing with Python* (The NLTK Book), Chapter 2, Section 5 (http://www.nltk.org/book/ch02.html)\n",
    "  * The `wordnet` module documentation (http://www.nltk.org/api/nltk.corpus.reader.html#module-nltk.corpus.reader.wordnet)\n",
    "  * The WordNet HOWTO for NLTK (http://www.nltk.org/howto/wordnet.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILL THIS IN FOR YOUR GROUP, also name your file as: tttv-w19-<group>-<name1>-<name2>.ipynb\n",
    "\n",
    "# Group        : D\n",
    "# Name - UvaID : Joshua de Roos - 11242736\n",
    "# Name - UvaID : Lodewijk van Keizerswaard - 11054115\n",
    "# Date         : 13 may 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the WordNet interface\n",
    "\n",
    "The first time that you use WordNet, you will need to download it. You may comment that line out after you have WordNet working on your machine (but it does no harm to leave it there).\n",
    "\n",
    "You will also need the `math` module for this assignment, and so let's load that now, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/joshua/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import *\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using WordNet\n",
    "\n",
    "The starting point of most WordNet queries involves pulling lists of synsets for particular words. NLTK allows these queries to be restricted to specific parts of speech: `wn.NOUN`, `wn.VERB`, or `wn.ADJ`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('bank.n.01'),\n",
       " Synset('depository_financial_institution.n.01'),\n",
       " Synset('bank.n.03'),\n",
       " Synset('bank.n.04'),\n",
       " Synset('bank.n.05'),\n",
       " Synset('bank.n.06'),\n",
       " Synset('bank.n.07'),\n",
       " Synset('savings_bank.n.02'),\n",
       " Synset('bank.n.09'),\n",
       " Synset('bank.n.10'),\n",
       " Synset('bank.v.01'),\n",
       " Synset('bank.v.02'),\n",
       " Synset('bank.v.03'),\n",
       " Synset('bank.v.04'),\n",
       " Synset('bank.v.05'),\n",
       " Synset('deposit.v.02'),\n",
       " Synset('bank.v.07'),\n",
       " Synset('trust.v.01')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('bank')\n",
    "\n",
    "# [Synset('bank.n.01'),\n",
    "#  Synset('depository_financial_institution.n.01'),\n",
    "#  Synset('bank.n.03'),\n",
    "#  Synset('bank.n.04'),\n",
    "#  Synset('bank.n.05'),\n",
    "#  Synset('bank.n.06'),\n",
    "#  Synset('bank.n.07'),\n",
    "#  Synset('savings_bank.n.02'),\n",
    "#  Synset('bank.n.09'),\n",
    "#  Synset('bank.n.10'),\n",
    "#  Synset('bank.v.01'),\n",
    "#  Synset('bank.v.02'),\n",
    "#  Synset('bank.v.03'),\n",
    "#  Synset('bank.v.04'),\n",
    "#  Synset('bank.v.05'),\n",
    "#  Synset('deposit.v.02'),\n",
    "#  Synset('bank.v.07'),\n",
    "#  Synset('trust.v.01')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('bank.n.01'),\n",
       " Synset('depository_financial_institution.n.01'),\n",
       " Synset('bank.n.03'),\n",
       " Synset('bank.n.04'),\n",
       " Synset('bank.n.05'),\n",
       " Synset('bank.n.06'),\n",
       " Synset('bank.n.07'),\n",
       " Synset('savings_bank.n.02'),\n",
       " Synset('bank.n.09'),\n",
       " Synset('bank.n.10')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('bank', pos = wn.NOUN)\n",
    "\n",
    "# [Synset('bank.n.01'),\n",
    "#  Synset('depository_financial_institution.n.01'),\n",
    "#  Synset('bank.n.03'),\n",
    "#  Synset('bank.n.04'),\n",
    "#  Synset('bank.n.05'),\n",
    "#  Synset('bank.n.06'),\n",
    "#  Synset('bank.n.07'),\n",
    "#  Synset('savings_bank.n.02'),\n",
    "#  Synset('bank.n.09'),\n",
    "#  Synset('bank.n.10')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Word similarity based on WordNet path length  (6 pts)\n",
    "\n",
    "Although NLTK provides `path_similarity()`, a function (and a method) for computing path-length similarity in WordNet, its definition is different than the definitions of path-length similarity in our textbook. In particular, NLTK only defines similarity between synsets, not whole words, and it normalises the similarity measure to fall between zero and one. \n",
    "\n",
    "Write a function `textbook_similarity()` that computes the similarity between two words, represented as Python strings, using the definitions of Equations 20.19 and 20.20 (2nd edition) or Equations 17.21 and 17.33 (3rd edition) in your textbook. In other words, return the maximum similarity across all pairs of senses of the two words, where similarity is defined to be `-log(shortest_path_length)`. Your function should include an argument to restrict the search to a specific part of speech. \n",
    "\n",
    "**N.B.:** Your function will require special treatment if the two words are the same (or part of the same synset), in which case the similarity should be `inf`, or if there is no path at all between the two words, in which case the similarity should be `-inf`. You may want to write a helper function to make this conversion for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inf\n",
      "-1.6094379124341003\n",
      "-1.3862943611198906\n",
      "-0.0\n",
      "-inf\n"
     ]
    }
   ],
   "source": [
    "def textbook_similarity(word1, word2, pos):\n",
    "    if word1 == word2:\n",
    "        return inf\n",
    "    \n",
    "    set1 = wn.synsets(word1, pos=pos)\n",
    "    set2 = wn.synsets(word2, pos=pos)\n",
    "    min_len= inf\n",
    "    \n",
    "    for sense1 in set1:\n",
    "        for sense2 in set2:\n",
    "            if sense1.root_hypernyms() != sense2.root_hypernyms():\n",
    "                continue\n",
    "                \n",
    "            path_len = sense1.shortest_path_distance(sense2)\n",
    "            if path_len < min_len:\n",
    "                min_len = path_len\n",
    "            \n",
    "    if min_len == inf:\n",
    "        return -min_len\n",
    "    return -log(min_len)\n",
    "\n",
    "print(textbook_similarity('port', 'port', wn.NOUN))  # Should be inf\n",
    "print(textbook_similarity('port', 'bank', wn.NOUN))  # Should be -1.61\n",
    "print(textbook_similarity('port', 'bank', wn.VERB))  # Should be -1.39\n",
    "print(textbook_similarity('port', 'drink', wn.VERB)) # Should be 0.00\n",
    "print(textbook_similarity('port', 'couch', wn.VERB)) # Should be -inf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: The Lesk family\n",
    "\n",
    "#### 2.1 Simplified Lesk (4 pts)\n",
    "\n",
    "Implement the *simplified* Lesk algorithm to disambiguate words in the context of single sentences. Implement the algorithm as a function that accepts a word and its context (the sentence). Your function should return an object of the NLTK WordNet `Synset` class.\n",
    "\n",
    "**Hint:** Use the pseudo-code in your textbook as a model. The NLTK documentation will help you extract glosses (definitions) and examples. You may need to experiment with the interface before writing your function, so that you understand exactly how NLTK returns information from WordNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('depository_financial_institution.n.01')\n"
     ]
    }
   ],
   "source": [
    "STOPWORDS = {\n",
    "    'a', 'able', 'about', 'across', 'after', 'all', 'almost', 'also', \n",
    "    'am', 'among', 'an', 'and', 'any', 'are', 'as', 'at', \n",
    "    'be', 'because', 'been', 'but', 'by', 'can', 'cannot', 'could', \n",
    "    'dear', 'did', 'do', 'does', 'either', 'else', 'ever', 'every', \n",
    "    'for', 'from', 'get', 'got', 'had', 'has', 'have', 'he', \n",
    "    'her', 'hers', 'him', 'his', 'how', 'however', 'i', 'if', \n",
    "    'in', 'into', 'is', 'it', 'its', 'just', 'least', 'let', \n",
    "    'like', 'likely', 'may', 'me', 'might', 'most', 'must', \n",
    "    'my', 'neither', 'no', 'nor', 'not', 'of', 'off', 'often', \n",
    "    'on', 'only', 'or', 'other', 'our', 'own', 'rather', 'said', \n",
    "    'say', 'says', 'she', 'should', 'since', 'so', 'some', 'than', \n",
    "    'that', 'the', 'their', 'them', 'then', 'there', 'these', 'they', \n",
    "    'this', 'tis', 'to', 'too', 'twas', 'us', 'wants', 'was', \n",
    "    'we', 'were', 'what', 'when', 'where', 'which', 'while', 'who', \n",
    "    'whom', 'why', 'will', 'with', 'would', 'yet', 'you', 'your'}\n",
    "\n",
    "def simplified_lesk(word, sentence):\n",
    "    set_word = wn.synsets(word)\n",
    "    best_sense = set_word[0]\n",
    "    max_overlap = 0\n",
    "    \n",
    "    context = set([x for x in set(sentence.split()) if x not in STOPWORDS and x != word])\n",
    "    \n",
    "    for sense in set_word:\n",
    "        words_example = []\n",
    "        gloss = [x for x in set(sense.definition().split()) if x not in STOPWORDS]\n",
    "        examples = sense.examples()\n",
    "        \n",
    "        \n",
    "        for example in examples:\n",
    "            words = [x for x in set(example.split()) if x not in STOPWORDS]\n",
    "            words_example = words_example + words\n",
    "\n",
    "        signature = set(gloss + words_example)\n",
    "        overlap = len(signature & context)\n",
    "        \n",
    "        if overlap > max_overlap:\n",
    "            max_overlap = overlap\n",
    "            best_sense = sense\n",
    "        \n",
    "    return best_sense\n",
    "\n",
    "\n",
    "\n",
    "# Example 20.10 from the textbook. \n",
    "# Should return Synset('depository_financial_institution.n.01')\n",
    "# Double check whether it is correct using 2.3\n",
    "print(simplified_lesk(\n",
    "    'bank',\n",
    "    'the bank can guarantee deposits will eventually cover future \\\n",
    "     tuition costs because it invests in adjustable-rate mortage securities'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Original Lesk (2 pts)\n",
    "\n",
    "Implement the *original* Lesk algorithm to disambiguate words in the context of single sentences. Implement the algorithm as a function that accepts a word and its context (the sentence).\n",
    "\n",
    "**Hint:** Use your simplified Lesk function as a basis. You may find it handy to write a helper function that computes signatures from word senses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('time.n.02')\n",
      "Synset('flies.n.01')\n",
      "Synset('arrow.n.02')\n"
     ]
    }
   ],
   "source": [
    "def original_lesk(word, sentence):\n",
    "    set_word = wn.synsets(word)\n",
    "    best_sense = set_word[0]\n",
    "    max_overlap = 0\n",
    "    \n",
    "    context = set([x for x in set(sentence.split()) if x not in STOPWORDS])\n",
    "    \n",
    "    for sense in set_word:\n",
    "        signature_sense = signature(sense, word)\n",
    "        \n",
    "        for word in context:\n",
    "            senses_context = wn.synsets(word)\n",
    "            \n",
    "            for sense_context in senses_context:\n",
    "                signature_context = signature(sense_context, word)\n",
    "                overlap = len(signature_sense & signature_context)\n",
    "                \n",
    "                if overlap > max_overlap:\n",
    "                    max_overlap = overlap\n",
    "                    best_sense = sense\n",
    "    return best_sense\n",
    "\n",
    "\n",
    "def signature(sense, word):\n",
    "    words_example = []\n",
    "    sense_str = remove_brackets(sense.definition())\n",
    "    gloss = [x for x in set(sense_str.split()) if x not in STOPWORDS]\n",
    "    examples = sense.examples()\n",
    "    for example in examples:\n",
    "        example = remove_brackets(example)\n",
    "        words = [x for x in set(example.split()) if x not in STOPWORDS]\n",
    "        words_example = words_example + words\n",
    "\n",
    "    return set(gloss + words_example)\n",
    "\n",
    "\n",
    "def remove_brackets(string):\n",
    "    string = string.replace('(', \"\")\n",
    "    return string.replace(')', \"\")\n",
    "\n",
    "# Exercise 20.4 from the textbook (and an example from the original Lesk paper)\n",
    "print(original_lesk('time', 'time flies like an arrow'))  # Should return time.n.02\n",
    "print(original_lesk('flies', 'time flies like an arrow')) # Should return flies.n.01\n",
    "print(original_lesk('arrow', 'time flies like an arrow')) # Should return arrow.n.02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Comparing the simplified and original Lesk algorithms (2 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('time.n.01')\n",
      "Synset('fly.v.08')\n",
      "Synset('arrow.n.01')\n"
     ]
    }
   ],
   "source": [
    "print(simplified_lesk('time', 'time flies like an arrow'))  # Should return time.n.01 \n",
    "print(simplified_lesk('flies', 'time flies like an arrow')) # Should return fly.v.08\n",
    "print(simplified_lesk('arrow', 'time flies like an arrow')) # Should return arrow.n.01\n",
    "# If the function returns other answers, check your simplified Lesk algorithm for mistakes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These Lesk variants disagree about every sense of the famous sentence 'Time flies like an arrow.' Examine their output. Do either of the algorithms disambiguate the sentence correctly? Which version do you think does a better job, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** \n",
    "\n",
    "We think that the Lesk Original (LO) does a better job at disambiguating since for two out of the three words it chooses a better sense than the Lesk Simplified (LS).\n",
    "\n",
    "1. For the word **time** LS chooses the synset 'an instance or single occasion for some event' and LO the synset 'a period of time considered as a resource under your control and sufficient to accomplish something'. Since in this sentence, time is viewed as a resource, the LO algorithm chooses the better sense.\n",
    "\n",
    "2. For the word **flies** LS chooses the better sense because it at least chooses a verb-sense and not a noun-sense ('flies' is clearly a verb in this sentence).\n",
    "\n",
    "3. For the word **arrow** LO chooses the better sense since it chooses 'a projectile with a straight thin shaft and an arrowhead on one end and stabilizing vanes on the other; intended to be shot from a bow', whereas LS chooses 'a mark to indicate a direction or relation'. In this case the sense chosen by LO is better.\n",
    "\n",
    "All in all, LO chooses better sensen for two out of three words compared to LS. So LO performs better in this case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
